<h1> Articles to Explore Adding </h1>
<ul>
<li> [Explaining softmax: a way to convert raw scores from the output of a NN to probabilities] 
(https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/softmax-regression.ipynb)  </li>
<li> Understanding Backpropagation: http://colah.github.io/posts/2015-08-Backprop/ </li>
<li> Understanding Cross-Entropy: 
https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a
 [Entropy is a measure of the uncertainty associated with a given distribution q(y).]</li>
<li> Understanding CNNs <ol> </li>

<li> Understanding Convolutions: http://colah.github.io/posts/2014-07-Understanding-Convolutions/ </li>
<li> Understanding Convolutions Networks: http://colah.github.io/posts/2014-07-Understanding-Convolutions/</li>
<li> Intuitively Understanding of CNN: 
https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 </li>
<li> Deeper Mathematical Dive into CNNs: https://arxiv.org/pdf/1603.07285.pdf [animations: https://github.com/vdumoulin/conv_arithmetic]</li>
</ol>
<li> Visual Information Theory: http://colah.github.io/posts/2015-09-Visual-Information/ </li>
<li> Comparison of Distrubations: The Kullback-Leibler Divergence,or “KL Divergence” for short, is a measure of dissimilarity between two distributions </li>
<li>Future Directions of ML Research: https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795</li>

</ul> 